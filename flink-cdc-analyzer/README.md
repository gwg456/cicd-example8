# ğŸ”¥ Flink CDC æ•°æ®å˜æ›´åˆ†æè§£å†³æ–¹æ¡ˆ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

åŸºäº **Apache Flink CDC** çš„å®æ—¶æ•°æ®å˜æ›´åˆ†æç³»ç»Ÿï¼Œä¸“é—¨ç”¨äºç›‘æ§å’Œåˆ†ææŒ‡å®šè¡¨çš„å¢åˆ æ”¹æ“ä½œï¼Œæä¾›å¼ºå¤§çš„æµå¤„ç†èƒ½åŠ›å’Œç²¾ç¡®ä¸€æ¬¡çš„æ•°æ®ä¿è¯ã€‚

### ğŸ¯ æ ¸å¿ƒç‰¹æ€§
- âœ… **å®æ—¶ CDC æµå¤„ç†**: åŸºäº Flink CDC çš„ä½å»¶è¿Ÿæ•°æ®æ•è·
- âœ… **ç²¾ç¡®ä¸€æ¬¡ä¿è¯**: Flink Checkpoint æœºåˆ¶ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
- âœ… **æŒ‡å®šè¡¨ç›‘æ§**: çµæ´»é…ç½®ç›‘æ§æ•°æ®åº“å’Œè¡¨
- âœ… **æµå¼æ•°æ®å¤„ç†**: æ”¯æŒå¤æ‚çš„æµå¼è®¡ç®—å’Œèšåˆ
- âœ… **å¤šç§æ•°æ®æº**: æ”¯æŒ MySQLã€PostgreSQLã€MongoDB ç­‰
- âœ… **å¤šç§æ•°æ®æ±‡**: æ”¯æŒ Kafkaã€Elasticsearchã€ClickHouse ç­‰

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®æºå±‚                                  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   MySQL 5.7+    â”‚    â”‚ PostgreSQL 11+  â”‚    â”‚ MongoDB 4+  â”‚  â”‚
â”‚  â”‚   (ç”¨æˆ·è¡¨)       â”‚    â”‚   (è®¢å•è¡¨)       â”‚    â”‚  (æ—¥å¿—è¡¨)   â”‚  â”‚
â”‚  â”‚   (å•†å“è¡¨)       â”‚    â”‚   (æ”¯ä»˜è¡¨)       â”‚    â”‚  (äº‹ä»¶è¡¨)   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼ (CDC å®æ—¶æ•è·)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Flink CDC å¤„ç†å±‚                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   CDC Source    â”‚â”€â”€â”€â–¶â”‚  æ•°æ®å¤„ç† Job    â”‚â”€â”€â”€â–¶â”‚   Sink å±‚   â”‚  â”‚
â”‚  â”‚ â€¢ MySQL CDC     â”‚    â”‚ â€¢ æ•°æ®è¿‡æ»¤       â”‚    â”‚ â€¢ Kafka     â”‚  â”‚
â”‚  â”‚ â€¢ PG CDC        â”‚    â”‚ â€¢ æ•°æ®è½¬æ¢       â”‚    â”‚ â€¢ ES        â”‚  â”‚
â”‚  â”‚ â€¢ Mongo CDC     â”‚    â”‚ â€¢ æ•°æ®èšåˆ       â”‚    â”‚ â€¢ MySQL     â”‚  â”‚
â”‚  â”‚ â€¢ æ–­ç‚¹ç»­ä¼        â”‚    â”‚ â€¢ çŠ¶æ€ç®¡ç†       â”‚    â”‚ â€¢ ClickHouseâ”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                               â”‚                                 â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Checkpoint     â”‚    â”‚   State Backend â”‚    â”‚ ç›‘æ§å‘Šè­¦     â”‚  â”‚
â”‚  â”‚ â€¢ çŠ¶æ€å¿«ç…§       â”‚    â”‚ â€¢ RocksDB       â”‚    â”‚ â€¢ Metrics   â”‚  â”‚
â”‚  â”‚ â€¢ æ•…éšœæ¢å¤       â”‚    â”‚ â€¢ Memory        â”‚    â”‚ â€¢ Alerts    â”‚  â”‚
â”‚  â”‚ â€¢ ä¸€è‡´æ€§ä¿è¯     â”‚    â”‚ â€¢ HDFS          â”‚    â”‚ â€¢ Dashboard â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼ (å®æ—¶è¾“å‡º)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        æ•°æ®æ¶ˆè´¹å±‚                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   å®æ—¶ API      â”‚    â”‚   æ•°æ®åˆ†æ       â”‚    â”‚  ä¸šåŠ¡åº”ç”¨   â”‚  â”‚
â”‚  â”‚ â€¢ REST API      â”‚    â”‚ â€¢ Grafana       â”‚    â”‚ â€¢ åŒæ­¥ä»»åŠ¡  â”‚  â”‚
â”‚  â”‚ â€¢ GraphQL       â”‚    â”‚ â€¢ Kibana        â”‚    â”‚ â€¢ å‘Šè­¦ç³»ç»Ÿ  â”‚  â”‚
â”‚  â”‚ â€¢ WebSocket     â”‚    â”‚ â€¢ ClickHouse    â”‚    â”‚ â€¢ æ•°æ®æ¹–   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### ç¯å¢ƒè¦æ±‚
```bash
- Apache Flink 1.17+
- Java 11+
- Maven 3.6+
- MySQL 5.7+ (å¯ç”¨ binlog)
- Kafka 2.8+ (å¯é€‰)
- Elasticsearch 7.15+ (å¯é€‰)
```

### MySQL CDC é…ç½®
```ini
# my.cnf é…ç½®
[mysqld]
# å¯ç”¨binlog
log-bin=mysql-bin
binlog-format=ROW
binlog-row-image=FULL

# è®¾ç½®server-id
server-id=1

# binlogä¿ç•™æ—¶é—´
binlog_expire_logs_seconds=604800  # 7å¤©

# å¯ç”¨GTID (æ¨è)
gtid_mode=ON
enforce_gtid_consistency=ON
```

### å¿«é€Ÿéƒ¨ç½²
```bash
# 1. å…‹éš†é¡¹ç›®
git clone flink-cdc-analyzer
cd flink-cdc-analyzer

# 2. ç¼–è¯‘é¡¹ç›®
mvn clean package -DskipTests

# 3. å¯åŠ¨ Flink é›†ç¾¤
./bin/start-cluster.sh

# 4. æäº¤ä½œä¸š
./bin/flink run -d \
  target/flink-cdc-analyzer-1.0.jar \
  --config-file conf/application.yaml

# 5. å¯åŠ¨ API æœåŠ¡
java -jar target/api-server-1.0.jar
```

## ğŸ“ é¡¹ç›®ç»“æ„

```
flink-cdc-analyzer/
â”œâ”€â”€ ğŸ“„ README.md                           # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ ğŸ“„ pom.xml                             # Mavené…ç½®
â”œâ”€â”€ ğŸ“ conf/                               # é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ application.yaml                   # ä¸»é…ç½®æ–‡ä»¶
â”‚   â”œâ”€â”€ flink-conf.yaml                    # Flinké…ç½®
â”‚   â””â”€â”€ log4j2.xml                         # æ—¥å¿—é…ç½®
â”œâ”€â”€ ğŸ“ src/main/java/                      # Javaæºä»£ç 
â”‚   â””â”€â”€ com/analytics/flinkcdc/
â”‚       â”œâ”€â”€ ğŸ“ source/                     # CDCæ•°æ®æº
â”‚       â”‚   â”œâ”€â”€ MysqlCdcSource.java        # MySQL CDCæº
â”‚       â”‚   â”œâ”€â”€ PostgresCdcSource.java    # PostgreSQL CDCæº
â”‚       â”‚   â””â”€â”€ MongoCdcSource.java        # MongoDB CDCæº
â”‚       â”œâ”€â”€ ğŸ“ processor/                  # æ•°æ®å¤„ç†å™¨
â”‚       â”‚   â”œâ”€â”€ DataFilterProcessor.java  # æ•°æ®è¿‡æ»¤
â”‚       â”‚   â”œâ”€â”€ DataTransformProcessor.java # æ•°æ®è½¬æ¢
â”‚       â”‚   â””â”€â”€ DataAggregateProcessor.java # æ•°æ®èšåˆ
â”‚       â”œâ”€â”€ ğŸ“ sink/                       # æ•°æ®æ±‡
â”‚       â”‚   â”œâ”€â”€ KafkaSink.java            # Kafkaè¾“å‡º
â”‚       â”‚   â”œâ”€â”€ ElasticsearchSink.java    # ESè¾“å‡º
â”‚       â”‚   â”œâ”€â”€ MysqlSink.java            # MySQLè¾“å‡º
â”‚       â”‚   â””â”€â”€ ClickHouseSink.java       # ClickHouseè¾“å‡º
â”‚       â”œâ”€â”€ ğŸ“ model/                      # æ•°æ®æ¨¡å‹
â”‚       â”‚   â”œâ”€â”€ ChangeEvent.java          # å˜æ›´äº‹ä»¶
â”‚       â”‚   â”œâ”€â”€ TableConfig.java          # è¡¨é…ç½®
â”‚       â”‚   â””â”€â”€ ProcessorConfig.java      # å¤„ç†å™¨é…ç½®
â”‚       â”œâ”€â”€ ğŸ“ util/                       # å·¥å…·ç±»
â”‚       â”‚   â”œâ”€â”€ ConfigUtil.java           # é…ç½®å·¥å…·
â”‚       â”‚   â”œâ”€â”€ JsonUtil.java             # JSONå·¥å…·
â”‚       â”‚   â””â”€â”€ TimeUtil.java             # æ—¶é—´å·¥å…·
â”‚       â””â”€â”€ ğŸ“ job/                        # Flinkä½œä¸š
â”‚           â”œâ”€â”€ MysqlCdcJob.java          # MySQL CDCä½œä¸š
â”‚           â”œâ”€â”€ MultiSourceCdcJob.java    # å¤šæºCDCä½œä¸š
â”‚           â””â”€â”€ RealTimeAnalyticsJob.java # å®æ—¶åˆ†æä½œä¸š
â”œâ”€â”€ ğŸ“ src/main/resources/                 # èµ„æºæ–‡ä»¶
â”‚   â”œâ”€â”€ META-INF/services/                # SPIé…ç½®
â”‚   â””â”€â”€ sql/                              # SQLè„šæœ¬
â”œâ”€â”€ ğŸ“ api-server/                         # APIæœåŠ¡å™¨
â”‚   â””â”€â”€ src/main/java/
â”‚       â””â”€â”€ com/analytics/api/
â”‚           â”œâ”€â”€ ApiServer.java            # APIæœåŠ¡ä¸»ç±»
â”‚           â”œâ”€â”€ controller/               # æ§åˆ¶å™¨
â”‚           â”œâ”€â”€ service/                  # æœåŠ¡å±‚
â”‚           â””â”€â”€ model/                    # æ•°æ®æ¨¡å‹
â”œâ”€â”€ ğŸ“ web-ui/                            # Webç•Œé¢
â”‚   â”œâ”€â”€ src/                              # å‰ç«¯æºç 
â”‚   â”œâ”€â”€ public/                           # é™æ€èµ„æº
â”‚   â””â”€â”€ package.json                      # Node.jsé…ç½®
â””â”€â”€ ğŸ“ docs/                              # æ–‡æ¡£
    â”œâ”€â”€ deployment.md                     # éƒ¨ç½²æ–‡æ¡£
    â”œâ”€â”€ configuration.md                  # é…ç½®æ–‡æ¡£
    â””â”€â”€ api.md                           # APIæ–‡æ¡£
```

## ğŸ”§ é…ç½®è¯´æ˜

### ä¸»é…ç½®æ–‡ä»¶
```yaml
# conf/application.yaml
flink:
  # Flink é›†ç¾¤é…ç½®
  cluster:
    job_manager: "localhost:8081"
    parallelism: 4
    checkpoint_interval: 5000
    checkpoint_mode: "EXACTLY_ONCE"
    
  # çŠ¶æ€åç«¯é…ç½®
  state_backend:
    type: "rocksdb"  # memory, filesystem, rocksdb
    checkpoint_dir: "hdfs://localhost:9000/flink/checkpoints"
    savepoint_dir: "hdfs://localhost:9000/flink/savepoints"

# MySQL CDC é…ç½®
mysql_cdc:
  # æ•°æ®åº“è¿æ¥
  connection:
    hostname: "localhost"
    port: 3306
    username: "flink_cdc"
    password: "FlinkCdc2024!"
    
  # CDC é…ç½®
  cdc:
    server_id: "5400-5404"
    server_time_zone: "UTC"
    scan_startup_mode: "initial"  # initial, earliest-offset, latest-offset, specific-offset, timestamp
    
  # ç›‘æ§è¡¨é…ç½®
  tables:
    - database: "ecommerce"
      table: "users"
      operations: ["INSERT", "UPDATE", "DELETE"]
      track_columns: ["id", "username", "email", "phone", "status"]
      sensitive_columns: ["phone", "email"]
      primary_key: ["id"]
      
    - database: "ecommerce"
      table: "orders"
      operations: ["INSERT", "UPDATE", "DELETE"]
      track_columns: ["id", "user_id", "amount", "status", "created_at"]
      primary_key: ["id"]
      
    - database: "ecommerce"
      table: "products"
      operations: ["INSERT", "UPDATE", "DELETE"]
      track_columns: ["id", "name", "price", "stock", "category_id"]
      primary_key: ["id"]

# PostgreSQL CDC é…ç½®
postgres_cdc:
  connection:
    hostname: "localhost"
    port: 5432
    username: "postgres"
    password: "postgres"
    database: "ecommerce"
    
  cdc:
    slot_name: "flink_cdc_slot"
    decoding_plugin_name: "pgoutput"

# æ•°æ®å¤„ç†é…ç½®
processing:
  # çª—å£é…ç½®
  window:
    type: "tumbling"  # tumbling, sliding, session
    size: "1 minute"
    
  # æ°´å°é…ç½®
  watermark:
    max_out_of_orderness: "10 seconds"
    idle_source_timeout: "30 seconds"
    
  # è¿‡æ»¤é…ç½®
  filter:
    enable_table_filter: true
    enable_operation_filter: true
    enable_column_filter: true

# è¾“å‡ºé…ç½®
sinks:
  # Kafka è¾“å‡º
  kafka:
    enabled: true
    bootstrap_servers: "localhost:9092"
    topic: "mysql-cdc-events"
    
  # Elasticsearch è¾“å‡º
  elasticsearch:
    enabled: true
    hosts: ["localhost:9200"]
    index: "mysql-cdc-{date}"
    
  # MySQL è¾“å‡º
  mysql:
    enabled: true
    url: "jdbc:mysql://localhost:3306/cdc_analytics"
    table: "change_events"
    
  # ClickHouse è¾“å‡º
  clickhouse:
    enabled: false
    url: "jdbc:clickhouse://localhost:8123/cdc_analytics"
    table: "change_events"

# ç›‘æ§é…ç½®
monitoring:
  # Metrics é…ç½®
  metrics:
    enabled: true
    reporters: ["prometheus"]
    
  # å‘Šè­¦é…ç½®
  alerts:
    enabled: true
    webhook_url: "http://localhost:8080/alerts"
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

### 1. MySQL CDC æ•°æ®æ•è·
```java
// MySQL CDC Source é…ç½®
public class MysqlCdcSource {
    
    public static SourceFunction<String> createSource(MysqlCdcConfig config) {
        return MySqlSource.<String>builder()
            .hostname(config.getHostname())
            .port(config.getPort())
            .databaseList(config.getDatabases())
            .tableList(config.getTables())
            .username(config.getUsername())
            .password(config.getPassword())
            .serverTimeZone(config.getServerTimeZone())
            .serverId(config.getServerId())
            .startupOptions(StartupOptions.initial())
            .deserializer(new JsonDebeziumDeserializationSchema())
            .build();
    }
}
```

### 2. æ•°æ®è¿‡æ»¤å’Œè½¬æ¢
```java
// æ•°æ®è¿‡æ»¤å¤„ç†å™¨
public class DataFilterProcessor extends ProcessFunction<String, ChangeEvent> {
    
    private final TableConfig tableConfig;
    
    @Override
    public void processElement(String value, Context ctx, Collector<ChangeEvent> out) {
        try {
            // è§£æDebeziumæ¶ˆæ¯
            JsonNode jsonNode = JsonUtil.parseJson(value);
            
            // æå–è¡¨ä¿¡æ¯
            String database = jsonNode.path("source").path("db").asText();
            String table = jsonNode.path("source").path("table").asText();
            
            // æ£€æŸ¥æ˜¯å¦ä¸ºç›®æ ‡è¡¨
            if (isTargetTable(database, table)) {
                ChangeEvent event = parseChangeEvent(jsonNode);
                
                // åº”ç”¨è¿‡æ»¤è§„åˆ™
                if (shouldProcess(event)) {
                    out.collect(event);
                }
            }
        } catch (Exception e) {
            log.error("å¤„ç†æ•°æ®å¤±è´¥", e);
        }
    }
    
    private boolean isTargetTable(String database, String table) {
        return tableConfig.getTables().stream()
            .anyMatch(t -> t.getDatabase().equals(database) && 
                          t.getTable().equals(table));
    }
}
```

### 3. å®æ—¶æ•°æ®èšåˆ
```java
// å®æ—¶ç»Ÿè®¡å¤„ç†
public class RealTimeAnalyticsJob {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // é…ç½®æ£€æŸ¥ç‚¹
        env.enableCheckpointing(5000);
        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);
        
        // MySQL CDC æº
        SourceFunction<String> mysqlSource = MysqlCdcSource.createSource(config);
        
        DataStream<ChangeEvent> changes = env
            .addSource(mysqlSource)
            .process(new DataFilterProcessor(tableConfig))
            .name("CDC Data Filter");
        
        // å®æ—¶ç»Ÿè®¡ - æ¯åˆ†é’Ÿç»Ÿè®¡å„è¡¨æ“ä½œæ¬¡æ•°
        DataStream<TableStats> stats = changes
            .keyBy(event -> event.getDatabase() + "." + event.getTable())
            .window(TumblingProcessingTimeWindows.of(Time.minutes(1)))
            .aggregate(new OperationCountAggregator())
            .name("Real-time Table Stats");
        
        // è¾“å‡ºåˆ°å¤šä¸ªSink
        changes.addSink(new KafkaSink<>(kafkaConfig)).name("Kafka Sink");
        changes.addSink(new ElasticsearchSink<>(esConfig)).name("ES Sink");
        stats.addSink(new MysqlSink<>(mysqlConfig)).name("MySQL Stats Sink");
        
        env.execute("MySQL CDC Real-time Analytics");
    }
}
```

### 4. å¤šæº CDC ä½œä¸š
```java
// å¤šæ•°æ®æºCDCä½œä¸š
public class MultiSourceCdcJob {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();
        
        // MySQL CDC æµ
        DataStream<ChangeEvent> mysqlStream = env
            .addSource(MysqlCdcSource.createSource(mysqlConfig))
            .process(new DataFilterProcessor(mysqlTableConfig))
            .map(event -> {
                event.setSource("mysql");
                return event;
            });
        
        // PostgreSQL CDC æµ
        DataStream<ChangeEvent> pgStream = env
            .addSource(PostgresCdcSource.createSource(pgConfig))
            .process(new DataFilterProcessor(pgTableConfig))
            .map(event -> {
                event.setSource("postgresql");
                return event;
            });
        
        // åˆå¹¶å¤šä¸ªæµ
        DataStream<ChangeEvent> unifiedStream = mysqlStream
            .union(pgStream)
            .keyBy(ChangeEvent::getTableKey)
            .process(new UnifiedChangeProcessor())
            .name("Unified Change Stream");
        
        // ç»Ÿä¸€è¾“å‡º
        unifiedStream.addSink(new KafkaSink<>(kafkaConfig));
        
        env.execute("Multi-Source CDC Job");
    }
}
```

## ğŸ“Š API æ¥å£

### REST API æœåŠ¡
```java
// API æ§åˆ¶å™¨
@RestController
@RequestMapping("/api/v1")
public class ChangeEventController {
    
    @Autowired
    private ChangeEventService changeEventService;
    
    // æŸ¥è¯¢è¡¨å˜æ›´è®°å½•
    @GetMapping("/tables/{database}/{table}/changes")
    public ResponseEntity<List<ChangeEvent>> getTableChanges(
            @PathVariable String database,
            @PathVariable String table,
            @RequestParam(defaultValue = "100") int limit,
            @RequestParam(required = false) String startTime,
            @RequestParam(required = false) String endTime) {
        
        QueryCriteria criteria = QueryCriteria.builder()
            .database(database)
            .table(table)
            .limit(limit)
            .startTime(startTime)
            .endTime(endTime)
            .build();
            
        List<ChangeEvent> changes = changeEventService.queryChanges(criteria);
        return ResponseEntity.ok(changes);
    }
    
    // è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
    @GetMapping("/tables/{database}/{table}/stats")
    public ResponseEntity<TableStats> getTableStats(
            @PathVariable String database,
            @PathVariable String table,
            @RequestParam(defaultValue = "7") int days) {
        
        TableStats stats = changeEventService.getTableStats(database, table, days);
        return ResponseEntity.ok(stats);
    }
    
    // å®æ—¶å˜æ›´æµ
    @GetMapping("/changes/stream")
    public SseEmitter getChangeStream() {
        return changeEventService.createChangeStream();
    }
}
```

### WebSocket å®æ—¶æ¨é€
```java
// WebSocket é…ç½®
@Component
public class RealTimeChangeHandler {
    
    @EventListener
    public void handleChangeEvent(ChangeEvent event) {
        // æ¨é€åˆ°WebSocketå®¢æˆ·ç«¯
        messagingTemplate.convertAndSend("/topic/changes", event);
    }
    
    @MessageMapping("/subscribe/table")
    public void subscribeTable(@DestinationVariable String database,
                              @DestinationVariable String table,
                              Principal principal) {
        // è®¢é˜…ç‰¹å®šè¡¨çš„å˜æ›´
        subscriptionService.subscribe(principal.getName(), database, table);
    }
}
```

## ğŸ” é«˜çº§åŠŸèƒ½

### 1. æ•°æ®è´¨é‡ç›‘æ§
```java
// æ•°æ®è´¨é‡æ£€æŸ¥
public class DataQualityProcessor extends ProcessFunction<ChangeEvent, QualityAlert> {
    
    @Override
    public void processElement(ChangeEvent event, Context ctx, Collector<QualityAlert> out) {
        // æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        if (hasNullPrimaryKey(event)) {
            out.collect(QualityAlert.builder()
                .type("NULL_PRIMARY_KEY")
                .table(event.getTableKey())
                .timestamp(event.getTimestamp())
                .build());
        }
        
        // æ£€æŸ¥æ•°æ®æ ¼å¼
        if (hasInvalidFormat(event)) {
            out.collect(QualityAlert.builder()
                .type("INVALID_FORMAT")
                .table(event.getTableKey())
                .details(getFormatErrors(event))
                .build());
        }
    }
}
```

### 2. è‡ªå®šä¹‰åºåˆ—åŒ–å™¨
```java
// è‡ªå®šä¹‰Debeziumååºåˆ—åŒ–å™¨
public class CustomDebeziumDeserializationSchema implements DebeziumDeserializationSchema<ChangeEvent> {
    
    @Override
    public void deserialize(SourceRecord record, Collector<ChangeEvent> out) throws Exception {
        // è§£æDebeziumè®°å½•
        Struct value = (Struct) record.value();
        Struct source = value.getStruct("source");
        
        // æ„å»ºå˜æ›´äº‹ä»¶
        ChangeEvent event = ChangeEvent.builder()
            .timestamp(Instant.ofEpochMilli((Long) source.get("ts_ms")))
            .database(source.getString("db"))
            .table(source.getString("table"))
            .operation(getOperation(value))
            .before(parseRowData(value.getStruct("before")))
            .after(parseRowData(value.getStruct("after")))
            .build();
            
        out.collect(event);
    }
    
    private String getOperation(Struct value) {
        String op = value.getString("op");
        switch (op) {
            case "c": return "INSERT";
            case "u": return "UPDATE";
            case "d": return "DELETE";
            default: return "UNKNOWN";
        }
    }
}
```

### 3. çŠ¶æ€ç®¡ç†å’Œå®¹é”™
```java
// çŠ¶æ€åŒ–å¤„ç†å™¨
public class StatefulChangeProcessor extends KeyedProcessFunction<String, ChangeEvent, ChangeEvent> {
    
    private ValueState<ChangeEvent> lastChangeState;
    
    @Override
    public void open(Configuration parameters) {
        ValueStateDescriptor<ChangeEvent> descriptor = 
            new ValueStateDescriptor<>("lastChange", ChangeEvent.class);
        lastChangeState = getRuntimeContext().getState(descriptor);
    }
    
    @Override
    public void processElement(ChangeEvent event, Context ctx, Collector<ChangeEvent> out) throws Exception {
        ChangeEvent lastChange = lastChangeState.value();
        
        // å»é‡é€»è¾‘
        if (lastChange == null || !isDuplicate(lastChange, event)) {
            lastChangeState.update(event);
            out.collect(event);
        }
        
        // è®¾ç½®å®šæ—¶å™¨æ¸…ç†è¿‡æœŸçŠ¶æ€
        ctx.timerService().registerProcessingTimeTimer(
            ctx.timerService().currentProcessingTime() + 3600000); // 1å°æ—¶åæ¸…ç†
    }
    
    @Override
    public void onTimer(long timestamp, OnTimerContext ctx, Collector<ChangeEvent> out) {
        lastChangeState.clear();
    }
}
```

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### Prometheus æŒ‡æ ‡é›†æˆ
```java
// è‡ªå®šä¹‰æŒ‡æ ‡
public class CdcMetrics {
    
    private static final Counter EVENTS_PROCESSED = Counter.build()
        .name("cdc_events_processed_total")
        .help("Total number of CDC events processed")
        .labelNames("database", "table", "operation")
        .register();
        
    private static final Histogram PROCESSING_LATENCY = Histogram.build()
        .name("cdc_processing_latency_seconds")
        .help("CDC event processing latency")
        .register();
    
    public static void recordEvent(String database, String table, String operation) {
        EVENTS_PROCESSED.labels(database, table, operation).inc();
    }
    
    public static void recordLatency(double latencySeconds) {
        PROCESSING_LATENCY.observe(latencySeconds);
    }
}
```

### å‘Šè­¦è§„åˆ™é…ç½®
```yaml
# å‘Šè­¦é…ç½®
alerts:
  rules:
    - name: "cdc_lag_high"
      condition: "cdc_lag_seconds > 300"
      severity: "warning"
      message: "CDCå¤„ç†å»¶è¿Ÿè¶…è¿‡5åˆ†é’Ÿ"
      
    - name: "event_processing_error_rate_high"
      condition: "error_rate > 0.05"
      severity: "critical"
      message: "äº‹ä»¶å¤„ç†é”™è¯¯ç‡è¶…è¿‡5%"
      
    - name: "checkpoint_failure"
      condition: "checkpoint_failure_count > 3"
      severity: "critical"
      message: "Checkpointè¿ç»­å¤±è´¥è¶…è¿‡3æ¬¡"
```

## ğŸ’¡ ä½¿ç”¨åœºæ™¯

### 1. å®æ—¶æ•°æ®åŒæ­¥
```java
// æ•°æ®åŒæ­¥ä½œä¸š
public class DataSyncJob {
    public static void main(String[] args) throws Exception {
        env.addSource(mysqlCdcSource)
           .process(new DataTransformProcessor())
           .addSink(new ClickHouseSink());
    }
}
```

### 2. å®æ—¶ä¸šåŠ¡ç›‘æ§
```java
// ä¸šåŠ¡æŒ‡æ ‡è®¡ç®—
DataStream<BusinessMetric> metrics = changes
    .filter(event -> "orders".equals(event.getTable()))
    .keyBy(event -> event.getAfter().get("user_id"))
    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
    .aggregate(new OrderMetricsAggregator());
```

### 3. æ•°æ®æ¹–æ„å»º
```java
// æ•°æ®æ¹–å†™å…¥
changes.addSink(
    StreamingFileSink.forRowFormat(
        new Path("hdfs://localhost:9000/datalake/cdc/"),
        new SimpleStringEncoder<ChangeEvent>()
    ).build()
);
```

---

ğŸš€ **åŸºäº Flink CDC çš„ä¸“ä¸šçº§æ•°æ®å˜æ›´åˆ†æè§£å†³æ–¹æ¡ˆï¼Œæä¾›ä¼ä¸šçº§çš„å®æ—¶æ•°æ®å¤„ç†èƒ½åŠ›ï¼**